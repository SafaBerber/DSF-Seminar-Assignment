{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn nltk torch -q evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a sentence classifier for ESG (Environmental, Social, and Governance) detection\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "punkt_params = PunktParameters()\n",
    "tokenizer_sent = PunktSentenceTokenizer(punkt_params)\n",
    "def safe_sent_tokenize(text):\n",
    "    return tokenizer_sent.tokenize(text)\n",
    "\n",
    "# Load and tokenize ESG sentence dataset\n",
    "dataset = load_dataset(\"climatebert/climate_detection\")\n",
    "\n",
    "model_name = \"climatebert/distilroberta-base-climate-detector\"\n",
    "tokenizer_esg = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer_esg(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Fine-tune ESG classifier\n",
    "model_esg = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./esg_classifier_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model_esg,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer_esg,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_esg),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"esg-sentence-classifier\")\n",
    "tokenizer_esg.save_pretrained(\"esg-sentence-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file esg-sentence-classifier/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"esg-sentence-classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no\",\n",
      "    \"1\": \"yes\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"no\": 0,\n",
      "    \"yes\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50500\n",
      "}\n",
      "\n",
      "loading weights file esg-sentence-classifier/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at esg-sentence-classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "/Users/dennysu/Desktop/Uni/Master/Data Science For Finance Seminar/Group Assignment/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/vocab.json\n",
      "loading file merges.txt from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/tokenizer.json\n",
      "loading file added_tokens.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"climatebert/distilroberta-base-climate-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"opportunity\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"risk\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"neutral\": 1,\n",
      "    \"opportunity\": 0,\n",
      "    \"risk\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50500\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/dennysu/.cache/huggingface/hub/models--climatebert--distilroberta-base-climate-sentiment/snapshots/e9f9a94ee4263f5ad5cfc97b8539a497fc88aa7d/model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at climatebert/distilroberta-base-climate-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Item_1...\n",
      "Processing Item_1A...\n",
      "Processing Item_7...\n",
      "Processing Item_8...\n",
      "âœ… ESG exposure + sentiment scores saved to: Data/Climate_bert/Climatebert_10K_filings_2023.csv\n"
     ]
    }
   ],
   "source": [
    "year = 2018\n",
    "\n",
    "tokenizer_esg = AutoTokenizer.from_pretrained(\"esg-sentence-classifier\")\n",
    "model_esg = AutoModelForSequenceClassification.from_pretrained(\"esg-sentence-classifier\")\n",
    "model_esg.eval()\n",
    "# Load sentiment model\n",
    "tokenizer_sentiment = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-sentiment\")\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(\"climatebert/distilroberta-base-climate-sentiment\")\n",
    "model_sentiment.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_esg.to(device)\n",
    "model_sentiment.to(device)\n",
    "\n",
    "# Define ESG + sentiment scoring function\n",
    "def compute_esg_exposure_and_sentiment(text, batch_size=32):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0, 0, 0.0\n",
    "\n",
    "    sentences = safe_sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences == 0:\n",
    "        return 0.0, 0, 0, 0.0\n",
    "\n",
    "    # -------- Batch ESG relevance classification --------\n",
    "    esg_probs = []\n",
    "    for i in range(0, total_sentences, batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer_esg(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_esg(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        esg_probs.extend(probs[:, 1].cpu().tolist())  # Probabilities for ESG class\n",
    "\n",
    "    # -------- Filter ESG-relevant sentences --------\n",
    "    esg_sentences = [sent for sent, prob in zip(sentences, esg_probs) if prob > 0.5]\n",
    "    esg_sentences_count = len(esg_sentences)\n",
    "\n",
    "    # -------- Batch sentiment scoring only for ESG sentences --------\n",
    "    sentiment_scores = []\n",
    "    for i in range(0, esg_sentences_count, batch_size):\n",
    "        batch = esg_sentences[i:i+batch_size]\n",
    "        inputs = tokenizer_sentiment(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_sentiment(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        batch_scores = (probs[:, 2] - probs[:, 0]).cpu().tolist()  # opportunity - risk\n",
    "        sentiment_scores.extend(batch_scores)\n",
    "\n",
    "    exposure_score = esg_sentences_count / total_sentences\n",
    "    avg_sentiment = np.mean(sentiment_scores) if sentiment_scores else 0.0\n",
    "\n",
    "    return exposure_score, esg_sentences_count, total_sentences, avg_sentiment\n",
    "\n",
    "items_path = f\"Data/Data_Cleaning/items_cleaned_10K_filings_{year}_1.csv\"\n",
    "output_path = f\"Data/Climate_bert/Climatebert_10K_filings_{year}.csv\"\n",
    "items_df = pd.read_csv(items_path)\n",
    "\n",
    "for item in [\"Item_1\", \"Item_1A\", \"Item_7\", \"Item_8\"]:\n",
    "    print(f\"Processing {item}...\")\n",
    "    results = items_df[item].apply(compute_esg_exposure_and_sentiment)\n",
    "    items_df[f\"{item}_ESG_Exposure\"] = results.apply(lambda x: x[0])\n",
    "    items_df[f\"{item}_ESG_Sentences\"] = results.apply(lambda x: x[1])\n",
    "    items_df[f\"{item}_Total_Sentences\"] = results.apply(lambda x: x[2])\n",
    "    items_df[f\"{item}_ESG_Sentiment\"] = results.apply(lambda x: x[3])\n",
    "\n",
    "# Compute overall metrics\n",
    "items_df[\"Overall_ESG_Exposure\"] = items_df[\n",
    "    [\"Item_1_ESG_Exposure\", \"Item_1A_ESG_Exposure\", \"Item_7_ESG_Exposure\", \"Item_8_ESG_Exposure\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "items_df[\"Overall_ESG_Sentiment\"] = items_df[\n",
    "    [\"Item_1_ESG_Sentiment\", \"Item_1A_ESG_Sentiment\", \"Item_7_ESG_Sentiment\", \"Item_8_ESG_Sentiment\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "columns_to_keep = [\n",
    "    \"year\", \"company\", \"cik\",\n",
    "    \"Item_1_ESG_Exposure\", \"Item_1_ESG_Sentiment\",\n",
    "    \"Item_1A_ESG_Exposure\", \"Item_1A_ESG_Sentiment\",\n",
    "    \"Item_7_ESG_Exposure\", \"Item_7_ESG_Sentiment\",\n",
    "    \"Item_8_ESG_Exposure\", \"Item_8_ESG_Sentiment\",\n",
    "    \"Overall_ESG_Exposure\", \"Overall_ESG_Sentiment\"\n",
    "]\n",
    "items_df[columns_to_keep].to_csv(output_path, index=False)\n",
    "print(f\"âœ… ESG exposure + sentiment scores saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
